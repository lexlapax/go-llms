# LLM Package

The `llm` package provides interfaces and implementations for interacting with Language Model providers such as OpenAI and Anthropic. It handles the communication with these services and provides a unified API across different providers.

## Core Components

### Domain

#### Message

```go
type Role string

const (
    RoleSystem    Role = "system"
    RoleUser      Role = "user"
    RoleAssistant Role = "assistant"
    RoleTool      Role = "tool"
)

type Message struct {
    Role    Role   `json:"role"`
    Content string `json:"content"`
}
```

The `Message` struct represents a message in a conversation with a language model, with different roles such as system, user, assistant, or tool.

#### Response

```go
type Response struct {
    Content string `json:"content"`
    // Additional fields like metadata may be available depending on the provider
}
```

The `Response` struct contains the content generated by the language model.

#### Token

```go
type Token struct {
    Text     string `json:"text"`
    Finished bool   `json:"finished"`
}
```

The `Token` struct represents a token in a streamed response from a language model, with a flag indicating whether it's the final token.

#### ResponseStream

```go
type ResponseStream <-chan Token
```

`ResponseStream` is a channel that receives tokens from a streaming response.

### Provider Interface

```go
type Provider interface {
    // Generate produces text from a prompt
    Generate(ctx context.Context, prompt string, options ...Option) (string, error)
    
    // GenerateMessage produces a response from a sequence of messages
    GenerateMessage(ctx context.Context, messages []Message, options ...Option) (Response, error)
    
    // GenerateWithSchema produces structured output conforming to a schema
    GenerateWithSchema(ctx context.Context, prompt string, schema interface{}, options ...Option) (interface{}, error)
    
    // Stream streams responses token by token
    Stream(ctx context.Context, prompt string, options ...Option) (ResponseStream, error)
    
    // StreamMessage streams a response from a sequence of messages
    StreamMessage(ctx context.Context, messages []Message, options ...Option) (ResponseStream, error)
}
```

The `Provider` interface defines methods for generating text and streaming responses from language models, with support for both simple prompts and message-based conversations.

### Request Options

```go
type Option func(*ProviderOptions)

type ProviderOptions struct {
    Temperature      float64
    MaxTokens        int
    StopSequences    []string
    TopP             float64
    FrequencyPenalty float64
    PresencePenalty  float64
}

// WithTemperature sets the temperature for generation
func WithTemperature(temp float64) Option

// WithMaxTokens sets the maximum number of tokens to generate
func WithMaxTokens(max int) Option

// WithStopSequences sets sequences that stop generation
func WithStopSequences(sequences ...string) Option

// WithTopP sets the top-p value for nucleus sampling
func WithTopP(topP float64) Option

// WithFrequencyPenalty sets the frequency penalty
func WithFrequencyPenalty(penalty float64) Option

// WithPresencePenalty sets the presence penalty
func WithPresencePenalty(penalty float64) Option
```

These request options configure the behavior of the language model for a specific request, such as the randomness of outputs, length limits, and more.

### Provider Options System

Go-LLMs also features an interface-based provider option system that allows configuring providers with common and provider-specific options:

```go
// Base interface for all provider options
type ProviderOption interface {
    // Identifies which provider type this option is for
    ProviderType() string
}

// Provider-specific option interfaces
type OpenAIOption interface {
    ProviderOption
    ApplyToOpenAI(provider interface{})
}

type AnthropicOption interface {
    ProviderOption
    ApplyToAnthropic(provider interface{})
}

type GeminiOption interface {
    ProviderOption
    ApplyToGemini(provider interface{})
}

type MockOption interface {
    ProviderOption
    ApplyToMock(provider interface{})
}
```

Common options that work across all providers:

```go
// Set custom API endpoint
baseURLOption := domain.NewBaseURLOption("https://custom-endpoint.example.com")

// Create a custom HTTP client with specific timeout
httpClient := &http.Client{
    Timeout: 30 * time.Second,
}
httpClientOption := domain.NewHTTPClientOption(httpClient)

// Set timeout to 15 seconds
timeoutOption := domain.NewTimeoutOption(15000) // milliseconds

// Set custom headers
headersOption := domain.NewHeadersOption(map[string]string{
    "X-Custom-Header": "custom-value",
})
```

Provider-specific options examples:

```go
// OpenAI organization ID
orgOption := domain.NewOpenAIOrganizationOption("org-123456")

// Anthropic system prompt
systemPromptOption := domain.NewAnthropicSystemPromptOption(
    "You are a helpful coding assistant specializing in Go programming.")

// Gemini generation config
generationConfigOption := domain.NewGeminiGenerationConfigOption().
    WithTemperature(0.7).
    WithTopK(40).
    WithMaxOutputTokens(1024)
```

For detailed documentation on the provider options system, see the [Provider Options Guide](/docs/user-guide/provider-options.md).

## Provider Implementations

### OpenAI Provider

```go
// Create a new OpenAI provider with basic configuration
provider := provider.NewOpenAIProvider(
    "your-api-key",
    "gpt-4o", // Model name
)

// Create a new OpenAI provider with provider options
provider := provider.NewOpenAIProvider(
    "your-api-key",
    "gpt-4o",
    domain.NewOpenAIOrganizationOption("org-123456"),
    domain.NewHTTPClientOption(&http.Client{Timeout: 30 * time.Second}),
)

// Generate text
response, err := provider.Generate(ctx, "What is the capital of France?")

// Generate with request options
response, err := provider.Generate(
    ctx,
    "Write a poem about programming.",
    domain.WithTemperature(0.7),
    domain.WithMaxTokens(200),
)

// Stream response
stream, err := provider.Stream(ctx, "Tell me a story.")
if err != nil {
    // Handle error
}

for token := range stream {
    fmt.Print(token.Text)
    if token.Finished {
        fmt.Println()
    }
}
```

The OpenAI provider supports all OpenAI models including GPT-3.5, GPT-4, and GPT-4o.

### Anthropic Provider

```go
// Create a new Anthropic provider with basic configuration
provider := provider.NewAnthropicProvider(
    "your-api-key",
    "claude-3-5-sonnet-latest", // Model name
)

// Create a new Anthropic provider with provider options
provider := provider.NewAnthropicProvider(
    "your-api-key",
    "claude-3-5-sonnet-latest",
    domain.NewAnthropicSystemPromptOption("You are a helpful assistant."),
    domain.NewAnthropicMetadataOption(map[string]string{
        "user_id": "user123",
    }),
)

// Message-based conversation
messages := []domain.Message{
    {Role: domain.RoleUser, Content: "What is the meaning of life?"},
}

response, err := provider.GenerateMessage(ctx, messages)
```

The Anthropic provider supports Claude models including Claude 3 Opus, Sonnet, and Haiku.

### Gemini Provider

```go
// Create a new Gemini provider with basic configuration
provider := provider.NewGeminiProvider(
    "your-api-key",
    "gemini-2.0-flash-lite", // Model name
)

// Create a new Gemini provider with provider options
provider := provider.NewGeminiProvider(
    "your-api-key",
    "gemini-2.0-flash-lite",
    domain.NewGeminiGenerationConfigOption().
        WithTemperature(0.7).
        WithTopK(40).
        WithMaxOutputTokens(1024),
    domain.NewBaseURLOption("https://custom-endpoint.example.com"),
)

// Generate text
response, err := provider.Generate(ctx, "What are the major features of Go?")

// Message-based conversation
messages := []domain.Message{
    {Role: domain.RoleUser, Content: "Tell me about machine learning."},
}
response, err := provider.GenerateMessage(ctx, messages)
```

The Gemini provider supports Google's Gemini models.

### Mock Provider

```go
// Create a mock provider with default responses
provider := provider.NewMockProvider()

// Create a mock provider with options
provider := provider.NewMockProvider(
    domain.NewHTTPClientOption(&http.Client{Timeout: 30 * time.Second}),
    domain.NewBaseURLOption("https://mock-api.example.com"),
)

// Customize the mock provider's behavior
provider.WithGenerateFunc(func(ctx context.Context, prompt string, options ...domain.Option) (string, error) {
    return "This is a custom mock response for: " + prompt, nil
})
```

The mock provider is useful for testing and development without making actual API calls.

## Multi Provider

The multi provider allows using multiple LLM providers together with different strategies:

```go
// Create providers with their specific options
openAIProvider := provider.NewOpenAIProvider(
    openaiKey,
    "gpt-4o",
    domain.NewOpenAIOrganizationOption("org-123456"),
)

anthropicProvider := provider.NewAnthropicProvider(
    anthropicKey,
    "claude-3-5-sonnet-latest",
    domain.NewAnthropicSystemPromptOption("You are a helpful assistant."),
)

geminiProvider := provider.NewGeminiProvider(
    geminiKey,
    "gemini-2.0-flash-lite",
    domain.NewGeminiGenerationConfigOption().WithTemperature(0.7),
)

// Create provider weights
providers := []provider.ProviderWeight{
    {Provider: openAIProvider, Weight: 1.0, Name: "openai"},
    {Provider: anthropicProvider, Weight: 1.0, Name: "anthropic"},
    {Provider: geminiProvider, Weight: 1.0, Name: "gemini"},
}

// Create a multi-provider with the fastest strategy
fastestProvider := provider.NewMultiProvider(providers, provider.StrategyFastest)

// Create a multi-provider with the primary strategy
primaryProvider := provider.NewMultiProvider(providers, provider.StrategyPrimary).
    WithPrimaryProvider(0) // Use first provider as primary

// Create a multi-provider with the consensus strategy
consensusProvider := provider.NewMultiProvider(providers, provider.StrategyConsensus)
```

Multi-provider strategies include:

- **StrategyFastest**: Returns the result from the provider that responds first
- **StrategyPrimary**: Uses a designated primary provider, falling back to others on failure
- **StrategyConsensus**: Attempts to find consensus among multiple providers' results

### Consensus Configuration

```go
// Create a multi-provider with custom consensus configuration
consensusProvider := provider.NewMultiProvider(providers, provider.StrategyConsensus).
    WithConsensusStrategy(provider.ConsensusSimilarity). // Use similarity-based consensus
    WithSimilarityThreshold(0.7) // Set similarity threshold to 70%
```

Consensus strategies include:

- **ConsensusMajority**: Uses simple majority voting (most common response)
- **ConsensusSimilarity**: Groups responses by similarity and chooses largest group
- **ConsensusWeighted**: Considers provider weights in addition to response similarity

## Example Usage

### Simple Text Generation

```go
// Create a provider
provider := provider.NewOpenAIProvider("your-api-key", "gpt-4o")

// Generate text
response, err := provider.Generate(context.Background(), "What is the capital of France?")
if err != nil {
    fmt.Printf("Error: %v\n", err)
    return
}

fmt.Printf("Response: %s\n", response)
```

### Message-Based Conversation

```go
// Create a provider
provider := provider.NewAnthropicProvider("your-api-key", "claude-3-5-sonnet-latest")

// Define messages
messages := []domain.Message{
    {Role: domain.RoleSystem, Content: "You are a helpful assistant."},
    {Role: domain.RoleUser, Content: "What is the meaning of life?"},
}

// Generate response
response, err := provider.GenerateMessage(context.Background(), messages)
if err != nil {
    fmt.Printf("Error: %v\n", err)
    return
}

fmt.Printf("Response: %s\n", response.Content)
```

### Streaming Response

```go
// Create a provider
provider := provider.NewOpenAIProvider("your-api-key", "gpt-4o")

// Stream response
stream, err := provider.Stream(context.Background(), "Tell me a story about a dragon.")
if err != nil {
    fmt.Printf("Error: %v\n", err)
    return
}

fmt.Println("Streaming response:")
for token := range stream {
    fmt.Print(token.Text)
    if token.Finished {
        fmt.Println()
    }
}
```
